{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.optimizer import *\n",
    "from local.learner import *\n",
    "from local.metrics import *\n",
    "from local.text.core import *\n",
    "from local.text.data import *\n",
    "from local.text.models.core import *\n",
    "from local.text.models.awdlstm import *\n",
    "from local.callback.rnn import *\n",
    "from local.callback.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration test on Wikitext-2\n",
    "\n",
    "> Training a Language Model on WT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.WIKITEXT_TINY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with all the wrticles concatenated. We split them to be able to shuffle at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def istitle(line):\n",
    "    return len(re.findall(r'^ = [^=]* = $', line)) != 0\n",
    "\n",
    "def read_file(filename):\n",
    "    articles = L()\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_article = ''\n",
    "    for i,line in enumerate(lines):\n",
    "        current_article += line.replace('<unk>', UNK)\n",
    "        if i < len(lines)-2 and lines[i+1] == ' \\n' and istitle(lines[i+2]):\n",
    "            articles.append(current_article.split(' '))\n",
    "            current_article = ''\n",
    "    articles.append(current_article.split(' '))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we put our list of tokenized texts together in an `LM_Dataset`. It will return tuples of sequences of `seq_len`, with the second sequence between the first one shifted by one on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_txt = read_file(path/'train.txt')\n",
    "val_txt = read_file(path/'valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter([p for t in trn_txt for p in t])\n",
    "vocab = make_vocab(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [list(range(len(val_txt), len(val_txt)+len(trn_txt))), list(range(len(val_txt)))]\n",
    "tfm = Numericalize(make_vocab(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource(val_txt+trn_txt, [tfm], filts=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,sl = 104,72\n",
    "train_dl = LMDataLoader(dsrc.train, bs=bs,   seq_len=sl, after_batch=[Cuda()], shuffle=True)\n",
    "valid_dl = LMDataLoader(dsrc.valid, bs=2*bs, seq_len=sl, after_batch=[Cuda()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n = Patriarchal Cathedral of the Holy Ascension of God = \\n \\n The Patriarchal Cathedral of the Holy Ascension of God ( Bulgarian : xxunk xxunk „ xxunk xxunk xxunk “ , xxunk xxunk „ xxunk xxunk xxunk “ ) is a former Eastern Orthodox cathedral in the city of xxunk Tarnovo , in north central Bulgaria . Located on top of the fortified Tsarevets hill in the former capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n \\n Some of Balliett 's \" real @-@ world ideas \" in Chasing Vermeer were \" Do coincidences mean anything ? \" and \" What is art and what makes it valuable ? \" Balliett says her \" central message \" is \" kids are powerful thinkers , and their ideas are valuable , and that adults don 't have all the answers . \" \\n A book by Rita xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allowing his friends time to use the collective consciousness to rebuild the guardian that had kept the beast trapped . However , in this process , Swamp Thing has his human soul removed , setting up the fourth run of the comic , relaunched shortly afterward . In the process John loses his memory , setting up the events leading up to the 200th issue . Leading up to the landmark issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tracks were included with other background music in the Snow Original Soundtrack released on April 25 , 2003 . Before the visual novel 's release , Snow Image Album was released at xxunk 63 on December 28 , 2002 . \\n Three drama CDs based on Snow have been published , the first CD volume was released by xxunk on August 22 , 2003 , focusing on Sumino Yukizuki . xxunk released</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trapped 1 @.@ 7 million birds , the largest number of any nuisance species to be destroyed . In 2005 , the population in the United States was estimated at 140 million birds , around 45 % of the global total of 310 million . \\n \\n = = = In science and culture = = = \\n \\n Common starlings may be kept as pets or as laboratory animals . Austrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>battalions now involved ( the 5th Battalion , Royal West xxunk had by now been tasked on the south east side of the village ) supported by tanks , Villa Grande was finally cleared by the end of 26 December . The troops of the 8th Indian Division entered the village to find a xxunk . One correspondent described the scene \" as though a giant had xxunk on a child 's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>at Lincoln 's Inn for three years . \\n \\n = = Marriage and family = = \\n \\n In November 1604 , he married Anne xxunk in a Protestant , Church of England ceremony at St Peter 's , Cornhill , where his address was registered as St Martin in the Fields . His children , including his eldest son and heir , Cecil , who was born in the winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Emmy Award for \" Most Outstanding Personality \" . The network 's other notable programs include : \\n Ted Mack 's The Original Amateur Hour , which began on radio in the 1930s under original host Edward Bowes \\n The Morey Amsterdam Show , a comedy / variety show hosted by Morey Amsterdam , which started on CBS before moving to DuMont in 1949 \\n Captain Video and His Video Rangers ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>. By 1964 , APF was the UK 's largest commercial user of colour film , consuming more than three million feet ( 570 miles or 910 kilometres ) of stock per year . \\n Alan Pattillo , a veteran xxunk and director for APF , was appointed the company 's first official script editor in late 1964 . This move was aimed to reduce the burden on Gerry Anderson who ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n = = Etymology = = \\n \\n The earliest named settlement within the domain of modern @-@ day Haifa was a city known as xxunk . Tel Shikmona Hebrew meaning \" mound of the xxunk xxunk \" ( Arabic Tell el @-@ xxunk or Tell es @-@ Samak , meaning \" mound of the fish \" ) preserved and transformed this ancient name and is mentioned once in the xxunk (</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbch = DataBunch(train_dl, valid_dl)\n",
    "dbch.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 16.3 ms, total: 4.06 s\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x,y in dbch.train_dl: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = awd_lstm_lm_config.copy()\n",
    "config.update({'input_p': 0.6, 'output_p': 0.4, 'weight_p': 0.5, 'embed_p': 0.1, 'hidden_p': 0.2})\n",
    "model = get_language_model(AWD_LSTM, len(vocab), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP():\n",
    "    def __init__(self, opt, i): \n",
    "        self.opt,self.i = opt,i\n",
    "    def __getitem__(self, k): \n",
    "        if k!= 'mom': return self.opt.param_groups[self.i][k]\n",
    "        else: return self.opt.param_groups[self.i]['betas'][0]\n",
    "    def __setitem__(self, k, v): \n",
    "        if k != 'mom': self.opt.param_groups[self.i][k] = v\n",
    "        else: self.opt.param_groups[self.i]['betas'] = (v,0.99)\n",
    "\n",
    "class AdamOpt():\n",
    "    def __init__(self, params, lr, wd=0., eps=1e-7):\n",
    "        self.opt = torch.optim.Adam(params, lr=lr, weight_decay=wd, eps=eps, betas=(0.9, 0.99)) \n",
    "        self.hypers = [HP(self.opt, i) for i in range(len(self.opt.param_groups))]\n",
    "        \n",
    "    def step(self): self.opt.step()\n",
    "    def zero_grad(self): self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = AdamOpt(model.parameters(), lr=5e-4, wd=0.1, eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Adam, wd=0.1, eps=1e-7)\n",
    "cb_funcs = [partial(MixedPrecision, clip=0.1), partial(RNNTrainer, alpha=2, beta=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, dbch, loss_func=CrossEntropyLossFlat(), opt_func=opt_func, cb_funcs=cb_funcs, metrics=[accuracy, Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.702378</td>\n",
       "      <td>6.909925</td>\n",
       "      <td>0.058589</td>\n",
       "      <td>1002.172546</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 5e-3, moms=(0.8,0.7,0.8), div=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
