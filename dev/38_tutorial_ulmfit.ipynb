{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.optimizer import *\n",
    "from local.learner import *\n",
    "from local.metrics import *\n",
    "from local.text.core import *\n",
    "from local.text.data import *\n",
    "from local.text.models.core import *\n",
    "from local.text.models.awdlstm import *\n",
    "from local.text.learner import *\n",
    "from local.callback.rnn import *\n",
    "from local.callback.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning in text\n",
    "\n",
    "> How to fine-tune a language model and train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a pretrained Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get our data and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df_tok,count = tokenize_df(df, 'text')\n",
    "texts,lens = df_tok['text'],df_tok['text_lengths'].values.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we put it in a `DataSource`. For a language model, we don't have targets, so there is only one transform to numericalize the texts. Note that `tokenize_df` returns the count of the words in the corpus to make it easy to create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(texts)\n",
    "vocab = make_vocab(count)\n",
    "dsrc = DataSource(texts, [Numericalize(vocab)], filts=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use that `DataSource` to create a `DataBunch`. Here the class of `TfmdDL` we need to use is `LMDataLoader` which will concatenate all the texts in a source (with a shuffle at each epoch for the training set), split it in `bs` chunks then read continuously through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = LMDataLoader.dbunchify(dsrc, lens=lens, bs=64, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxup the xxup devil 's xxup plaything is my second attempt at a xxmaj joseph xxmaj xxunk production - and although i will say it is far more enjoyable than the painfully dull and xxunk xxmaj swedish xxup xxunk , it is still a little slow and un - explicit for my taste . \\n\\n xxmaj this one centers around a group of vampire girls who live in a castle ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other poor countries have films with real characters that challenge the views of their respective societies and we just keep on xxunk out garbage . take a look at xxmaj russia , xxmaj iran , china and xxmaj xxunk xxmaj america , look at the brilliant films they have and we get crap like xxmaj xxunk ! ! \\n\\n get real people , no wonder the international community in general laughs at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>killing for the movie , feature fine acting from him and xxmaj amy xxmaj xxunk , xxmaj van 's daughter , as a supportive nurse . xxmaj caan was one of the 1970s ' best actors , and his xxunk xxunk with xxmaj xxunk , xxmaj duvall , xxmaj hopkins , and both xxmaj xxunk give \" killer xxmaj elite \" real xxunk . \\n\\n xxmaj but you do n't watch \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxmaj the xxmaj bourne xxmaj ultimatum - xxmaj jason xxmaj bourne ( matt xxmaj damon in his best role ever ) , the xxunk spy kid on the block , brings his quest for his identity to a close as he also seeks to end the cia 's latest program \" xxunk \" to make super xxunk like himself . \\n\\n i was so xxunk for this one that i watched it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of the humor . i do n't really care for his stand - up either . xxmaj but he makes some good points on the show now and again , and i liked xxmaj politically xxmaj incorrect , though he was still fairly politically correct ( which i xxunk a negative because the very term sounds xxmaj orwellian or at least xxunk ) . xxmaj as for my religious views , xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxmaj another sequel ? i hope so ! xxbos i am glad to read so many negative comments about the xxmaj tritter plot . xxmaj everyone i talk to says the same thing . xxmaj they like xxmaj house 's gruff nature and his intelligence , but really dislike the vindictiveness of this xxunk plot . xxmaj it cuts into the real nature of the hospital story and makes everyone angry at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 minutes , they were pretty funny . xxmaj and here , for about 20 minutes , it 's really funny . xxmaj what i thought was good about the character in the movie , is that he stayed in character throughout . xxmaj he never xxunk from his wanted to just get laid persona . xxmaj until right at the end where there was this transformation , and the ever present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>very xxunk ( at best ) , as if it were intended to not set itself apart from any other production . xxmaj but remember , again , this is from the artistic perspective of a theatrical director , not a dancer or a choreographer , but a straight male theatrical director . xxbos xxmaj sam xxmaj mraovich should never be allowed to touch a camera again . xxmaj if he does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>person she used to be \" and the xxunk murder caused by panic . xxmaj but you can also find funny stuff like intestines pulled through someone 's ass and a guy running in the woods then finding himself decapitated by a wire tied between two trees ( that makes a xxunk do xxrep 3 i xxunk sound afterward , like in cartoons ) . xxmaj somehow there is a market for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i can accept the xxunk , xxunk suitors but xxup please for the love of xxmaj god entertain me just a little . xxmaj no such luck . \\n\\n xxmaj if you 're expecting drama , intrigue , xxunk , or excitement you will be xxup severely disappointed . xxmaj the biggest \" drama \" comes from the fact that one of the suitors still may have a boyfriend in xxmaj new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbunch.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(dbunch, AWD_LSTM, vocab, metrics=[accuracy, Perplexity()], path=path, opt_func = partial(Adam, wd=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.578041</td>\n",
       "      <td>4.074596</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>58.826706</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze()\n",
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.334995</td>\n",
       "      <td>4.074596</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>58.826706</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.335794</td>\n",
       "      <td>4.074596</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>58.826706</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.333689</td>\n",
       "      <td>4.074596</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>58.826706</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.326313</td>\n",
       "      <td>4.074596</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>58.826706</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, 1e-2, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fine-tuned the pretrained language model to this corpus, we save the encoder since we will use it for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it to train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we need to use two set of transforms: one to numericalize the texts and the other to encode the labels as categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(df_tok))\n",
    "dsrc = DataSource(df_tok, filts=splits, tfms=[\n",
    "    [attrgetter(\"text\"), Numericalize(vocab)],\n",
    "    [attrgetter(\"label\"), Categorize()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opnce again use a subclass of `TfmdDL` for the dataloaders, since we want to sort the texts (sortish for the training set) by order of lengths. We also use `pad_collate` to create batches form texts of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = SortedDL(dsrc.train, create_batch=pad_collate, after_batch=[Cuda], shuffle=True, drop_last=True)\n",
    "val_dl = SortedDL(dsrc.valid, create_batch=pad_collate, after_batch=[Cuda])\n",
    "dbunch = DataBunch(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n\\n xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after xxunk years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \\n\\n xxmaj none of this excuses him this present , almost diabolical failure . xxmaj as xxmaj david xxmaj stratton xxunk , \" two parts of xxmaj che do n't ( even ) make a whole \" . \\n\\n xxmaj epic xxunk in name only , xxmaj che(2008 ) barely qualifies as a feature film ! xxmaj it certainly has no legs , xxunk as except for its xxunk ultimate resolution forced upon it by history , xxmaj soderbergh 's xxunk - long xxunk just goes nowhere . \\n\\n xxmaj even xxmaj margaret xxmaj xxunk , the more xxunk of xxmaj australia 's xxmaj at xxmaj the xxmaj movies duo , noted about xxmaj soderbergh 's xxunk waste of ( xxunk digital xxunk ) : \" you 're in the woods … you 're in the woods … you 're in the woods … \" . i too am surprised xxmaj soderbergh did n't give us another xxunk of xxup that somewhere between his xxunk two xxmaj parts , because he still left out massive xxunk of xxmaj che 's \" xxunk \" life ! \\n\\n xxmaj for a xxunk of an important but infamous historical figure , xxmaj soderbergh xxunk xxunk , if not deliberately insults , his audiences by \\n\\n 1 . never providing most of xxmaj che 's story ; \\n\\n 2 . xxunk xxunk film xxunk with mere xxunk xxunk ; \\n\\n 3 . xxunk both true hindsight and a narrative of events ; \\n\\n 4 . barely developing an idea , or a character ; \\n\\n 5 . remaining xxunk episodic ; \\n\\n 6 . xxunk proper context for scenes xxrep 3 - whatever we do get is xxunk in xxunk xxunk ; \\n\\n 7 . xxunk xxunk all audiences ( even xxmaj spanish - xxunk will be confused by the xxunk xxunk in xxmaj english ) ; and \\n\\n 8 . xxunk xxunk his main subject into one dimension . xxmaj why , at xxup this late stage ? xxmaj the xxmaj t - shirt franchise has been a success ! \\n\\n xxmaj our sense of xxunk is surely due to xxmaj peter xxmaj xxunk and xxmaj benjamin xxunk xxmaj xxunk xxunk their screenplay solely on xxmaj xxunk 's memoirs . xxmaj so , like a poor student who has read only xxup one of his xxunk xxunk for his xxunk , xxmaj soderbergh 's product is xxunk limited in perspective . \\n\\n xxmaj the audience is held captive within the same xxunk knowledge , scenery and circumstances of the \" revolutionaries \" , but that does n't xxunk our sympathy . xxmaj instead , it xxunk on us that \" ah , xxmaj soderbergh 's trying to xxunk his audiences the same as the xxmaj latino peasants were at the time \" . xxmaj but these are the xxup same illiterate xxmaj latino peasants who sold out the good doctor to his enemies . xxmaj why does xxmaj soderbergh feel the need to xxunk us with them , and keep us equally mentally captive ? xxmaj such audience xxunk must have a purpose . \\n\\n xxmaj part2 is more xxunk than xxmaj part1 , but it 's literally mind - numbing with its repetitive bush - bashing , misery of outlook , and lack of variety or character xxunk . deltoro 's xxmaj che has no opportunity to grow as a person while he struggles to xxunk his own ill - xxunk troops . xxmaj the only xxunk is the humour as xxmaj che deals with his sometimes deeply ignorant \" revolutionaries \" , some of whom xxunk lack self - control around local peasants or food . xxmaj we certainly get no insight into what caused the conditions , nor any xxunk xxunk of their xxunk xxunk , such as it was . \\n\\n xxmaj part2 's excruciating xxunk remains xxunk episodic : again , nothing is telegraphed or xxunk . xxmaj thus even the scenes with xxmaj xxunk xxmaj xxunk ( xxunk xxmaj xxunk ) are unexpected and disconcerting . xxmaj any xxunk events are portrayed xxunk and xxmaj latino - xxunk , with xxmaj part1 's interviews replaced by time - xxunk xxunk between the corrupt xxmaj xxunk president ( xxunk de xxmaj xxunk ) and xxup us xxmaj government xxunk promising xxup cia xxunk ( ! ) . \\n\\n xxmaj the rest of xxmaj part2 's \" woods \" and day - for - night blue xxunk just xxunk the audience until they 're xxunk the xxunk . \\n\\n xxmaj perhaps deltoro felt too xxunk the frustration of many non - american xxmaj latinos about never getting a truthful , xxunk history of xxmaj che 's exploits within their own countries . xxmaj when foreign xxunk still wo n't deliver a free press to their people -- for whatever reason -- then one can see how a popular xxmaj american indie producer might set out to entice the not - so - well - read ( \" i may not be able to read or write , but xxmaj i 'm xxup not xxunk xxmaj inspector xxmaj xxunk ) ) out to their own local cinemas . xxmaj the film 's obvious xxunk and gross over - xxunk hint very strongly that it 's aiming only at the xxunk of the less - informed xxup who xxup still xxup speak xxup little xxmaj english . xxmaj if they did , they 'd have read xxunk on the subject already , and xxunk the relevant social issues amongst themselves -- learning the lessons of history as they should . \\n\\n xxmaj such insights are precisely what societies still need -- and not just the remaining illiterate xxmaj latinos of xxmaj central and xxmaj south xxmaj america -- yet it 's what xxmaj che(2008 ) xxunk fails to deliver . xxmaj soderbergh xxunk his lead because he 's weak on narrative . i am xxunk why xxmaj xxunk deltoro deliberately chose xxmaj soderbergh for this project if he knew this . xxmaj it 's been xxunk , hindsight about xxmaj xxunk was xxunk wanted : it 's what i went to see this film for , but the director xxunk robs us of that . \\n\\n xxmaj david xxmaj stratton , writing in xxmaj the xxmaj australian ( xxunk ) observed that while xxmaj part1 was \" uneven \" , xxmaj part2 actually \" goes rapidly downhill \" from there , \" xxunk xxmaj che 's final campaign in xxmaj xxunk in excruciating detail \" , which \" … feels almost unbearably slow and turgid \" . \\n\\n che : the xxmaj xxunk aka xxmaj part2 is certainly no xxunk for xxmaj xxunk , painting it a picture of misery and xxunk . xxmaj the entire second half is only redeemed by the aforementioned humour , and the dramatic -- yet tragic -- capture and execution of the film 's subject . \\n\\n xxmaj the rest of this xxunk cinema xxunk is just confusing , irritating misery -- xxunk , for a xxmaj soderbergh film , to be avoided at all costs . xxmaj it is bound to break the hearts of all who know even just a xxunk about the xxunk / 10 )</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj this film sat on my xxmaj xxunk for weeks before i watched it . i xxunk a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj xxunk . \\n\\n xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" la xxmaj xxunk , \" based on a play by xxmaj arthur xxmaj xxunk , who is given an \" inspired by \" credit . xxmaj it starts from one person , a prostitute , standing on a street corner in xxmaj brooklyn . xxmaj she is picked up by a home contractor , who has sex with her on the hood of a car , but ca n't come . xxmaj he refuses to pay her . xxmaj when he 's off xxunk , she answers his cell phone and takes a message . xxmaj she runs away with his keys . \\n\\n xxmaj then the story switches to the contractor , who pays a professional call on a rich , bored xxmaj new xxmaj york woman , who plays with him until he is xxunk , then she pulls away . xxmaj she tells him how desperate and unhappy she is ; he tells her how beautiful she is , and lucky . xxmaj as he is leaving , she asks if he would have sex with her . xxmaj she sits on top of him , xxunk up and down . xxmaj this time he comes , the he leaves . \\n\\n xxmaj the woman and her husband throw a dinner party for their xxunk friends . xxmaj xxunk ( robert ) is talking business , wife ( ellen ) is bored , and switches the subject to sex , and how often men and women think about it . xxmaj husband switches conversation to desert . xxmaj later , after the xxunk leave , xxmaj ellen tries to entice xxmaj robert into sex . xxmaj robert wants none of it , and puts on a jazz record . xxmaj ellen turns on the radio ; xxmaj robert turns up the music ; xxmaj ellen turns on the xxup tv ; xxmaj robert turns on another xxup tv . xxmaj xxunk ensues . xxmaj ellen goes up on the roof , xxmaj robert joins her . xxmaj ellen confesses that she needs to experience more men , men other than xxmaj robert . xxmaj robert says that he too needs to experience men . \\n\\n xxmaj we next follow xxmaj robert as he visits an artist , xxmaj martin , played by xxmaj steve xxmaj buscemi . i wish xxmaj buscemi could have more roles like this , where he is a sexy , smart , totally xxunk guy . xxmaj robert xxunk xxmaj martin 's work , much more than it deserves , xxunk to get it into a show . xxmaj martin is excited , until it turns out that xxmaj robert is speaking out of his xxunk , it is all a xxunk dance . xxmaj robert tries to kiss xxmaj martin , on the lips , and xxmaj martin pulls back , saying that he is not gay . xxmaj robert xxunk that he 's not gay either , xxmaj martin xxunk . xxmaj both admit that the xxunk are bad . xxmaj robert is about to leave , when xxmaj martin allows xxmaj robert to kiss him . xxmaj they make out , and xxmaj robert goes down on xxmaj martin . \\n\\n xxmaj next we follow xxmaj martin , as he xxunk for an art show at a xxmaj manhattan gallery . xxmaj he is xxunk by the xxunk , xxmaj anna , played by xxmaj rosario xxmaj dawson . ( i had to cut some of this review to keep it under 1 xxrep 3 0 words ) … and they make love to each other . \\n\\n xxmaj we next follow xxmaj anna , who is sitting at a lunch stand . xxmaj her boyfriend , xxmaj nick ( adrian xxmaj xxunk ) , enters , bearing xxunk . xxmaj she is cold toward him ; he tries to figure out why . xxmaj he coaxes out of her the information that she has had sex with someone while he was in xxmaj san xxmaj francisco . xxmaj she coaxes out of him the fact that he has stayed with his ex - xxunk while in xxmaj san xxmaj francisco , and had sex with her . xxmaj the latter revelation turns out to be a lie . xxmaj the two of them make out in the xxunk , but she decides that they must break up . xxmaj nick is xxunk . \\n\\n xxmaj and we follow xxmaj nick , who confesses his xxunk to an older woman who he meets on a park bench , xxmaj joey ( carol xxmaj kane ) . xxmaj joey is sort of weird and child - like , but is a good audience for xxmaj nick , who needs a sympathetic ear . xxmaj the two of them go to xxmaj xxunk xxmaj island at night , and look at the stars . xxmaj nick falls under xxmaj joey 's spell , despite the age difference between them . xxmaj they go back to xxmaj joey 's apartment , and xxmaj nick gradually realizes that he is about to have sex with a crazy old woman . xxmaj she is on top of him , does n't want to let him go . xxmaj but he manages to escape . \\n\\n ( this is , by the way , the best xxmaj carol xxmaj kane role since she played xxmaj xxunk 's wife in xxmaj taxi . ) xxmaj joey 's phone rings , and it is a man calling the xxmaj psychic xxmaj friends xxmaj network , and xxmaj joey is one of the psychic friends . xxmaj although she is still hurting from xxmaj nick , she gradually gets into her psychic xxunk . xxmaj the man is at his office , late at night , and wants to have phone sex with her . xxmaj although that is not xxmaj joey 's business , xxmaj joey goes along , and coaxes the man to come . xxmaj she wants to keep talking , although the man want to get off the phone , and finds out that he has xxunk a lot of money from his company , and will be found out xxunk . xxmaj his life is ruined . xxmaj joey realizes that the man is going to commit suicide , and she tries to make him believe that she is his friend , that she cares about him . xxmaj and she does care about him . \\n\\n xxmaj but the man packs a gun into his xxunk , and goes off to seek a prostitute on the xxmaj brooklyn xxunk , and we come back to the beginning , to the same prostitute who started out xxmaj la xxmaj xxunk . xxmaj she wants to give him $ xxunk , xxrep 3 0 in cash if she will kill him . xxmaj he tried to kill himself , but could n't do it . xxmaj the prostitute does not want to do it , but he insists , holding her hand , holding the gun inside his mouth , telling her where to aim . xxmaj eventually , the gun goes off , and we see the prostitute walking down the street , and xxunk at the corner where she normally does business . xxmaj the contractor who did n't pay her earlier in the movie drives up , xxunk down the window . xxmaj they look at each other . xxup the xxup end .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbunch.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we once again have a convenience function to create a classifier from this `DataBunch` with the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dbunch, AWD_LSTM, vocab, metrics=[accuracy], path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our pretrained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train with gradual unfreezing and differential learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.849841</td>\n",
       "      <td>0.601815</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.753493</td>\n",
       "      <td>0.568468</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.753582</td>\n",
       "      <td>0.548259</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.740121</td>\n",
       "      <td>0.533096</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.710222</td>\n",
       "      <td>0.536359</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.715015</td>\n",
       "      <td>0.531864</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.737472</td>\n",
       "      <td>0.531057</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.682374</td>\n",
       "      <td>0.526255</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.531820</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.723326</td>\n",
       "      <td>0.524388</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.685744</td>\n",
       "      <td>0.527304</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.732956</td>\n",
       "      <td>0.520143</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, np.array([1e-5,3e-4,1e-4,3e-3,1e-3]), moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
